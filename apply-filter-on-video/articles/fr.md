# Appliquez des filtres sur vos vid√©os avec Javascript

Que ce soit pour am√©liorer la nettet√© d'une image, rehausser sa luminosit√©, modifier la r√©partition des couleurs, ou pour des centaines de raisons toutes aussi fond√©es, la plupart des images affich√©es par nos √©crans subissent de nombreux traitements.

En r√®gle g√©n√©rale, ces corrections s'appliquent avant l'enregistrement des images sous forme de fichier (pour des raisons d'optimisations √©videntes). Cela dit, il est parfois n√©cessaire d'appliquer des filtres sur un contenu vid√©o en temps r√©el.

## Comment retoucher des images √† la vol√©e ?

Je vous propose une solution tr√®s simple permettant d'effectuer ces traitements directement depuis notre navigateur. 

Cette m√©thode peut se r√©sumer ainsi :
- Int√©grer une vid√©o la page, gr√¢ce √† la balise `<video>` (id√©alement en cach√©)
- R√©cup√©rer son flux vid√©o dans un objet `ImageData`
- Effectuer le traitement de l'image dessus
- Afficher le r√©sultat dans une balise `<canvas>`.


### Int√©grer une vid√©o √† la page

```html
 <video
   id="tuto-video"
   src="your-video-url"
   width="300"
   height="300"
   controls
></video>
```


### R√©cup√©ration du flux vid√©o en JS

Si vous vous √™tes d√©j√† int√©ress√©s √† l'**encodage des vid√©os**, vous savez qu'obtenir un flux de pixels √† partir d'un fichier est une t√¢che complexe. Pour parser les fichiers vid√©os, ma√Ætriser les diff√©rents formats est un indispensable.

*¬´ - Alors, nous allons devoir apprendre tous les formats vid√©os pour continuer ? ¬ª* üò® 

Non, revenez ! Le navigateur s'occupe de tout ! 

Avec la balise `<video>`, extraire les pixels d'une vid√©o se fait en seulement quelques lignes :


```js
const video = document.getElementById('tuto-video');

// Create canvas for video's pixel extraction
const extractPixelCanvas = document.createElement('canvas');
const extractPixelContext = extractPixelCanvas.getContext('2d');

/**
 * @param {HTMLVideoElement} video
 * @param {Number} width 
 * @param {Number} height 
 * @return {ImageData} the pixel matrix
 */
function extractVideoImageData(video, width, height) {
    // avoid unnecessary resize as much as possible (optimization)
    if (extractPixelCanvas.width !==  width) {
        extractPixelCanvas.width =  width;
    }

    if (extractPixelCanvas.height !==  height) {
        extractPixelCanvas.height = height;
    }
    
    extractPixelContext.drawImage(video, 0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
    return extractPixelContext.getImageData(0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
}
```


### Manipuler ses pixels avec `ImageData`

Pour afficher le contenu d'une matrice de pixel sur un √©cran, il suffit de l'injecter dans un contexte canvas : 
```js
canvasContext2D.putImageData(instanceOfImageData, 0, 0);
```

#### Quelques explications sur la classe `ImageData`
La structure de l'objet est relativement simple :
* une instance poss√®de les propri√©t√©s `width` et `height`correspondant √† la r√©solution de l'image.
* Les pixels sont stock√©s dans la propri√©t√© `data`, une matrice de type `Uint8ClampedArray`.
* Les pixels sont encod√©s sous forme `RGBA`. L'*alpha* est compris entre 0 et 255.

Pour modifier une image √† la vol√©e, on modifiera les pixels contenus dans `data`.

Un exemple tir√© de [la documentation](https://developer.mozilla.org/en-US/docs/Web/API/ImageData/ImageData).

```js
// Iterate through every pixel
for (let i = 0; i < imageData.data; i += 4) {
    imageData.data[i + 0] = 0;    // R value
    imageData.data = 190;  // G value
    imageData.data = 0;    // B value
    imageData.data  // A value
}
```


### Afficher une image retouch√©e dans un `<canvas>`

```html
<!--html-->
<canvas id="tuto-canvas"></canvas>
```
```js
//js
const canvas = document.getElementById('tuto-canvas');
canvasContext2D = canvas.getContext('2d');

const instanceOfImageData = applyYourAmazingFilter(/* ... */);

canvasContext2D.putImageData(instanceOfImageData, 0, 0);
```

## Un filtre, oui ; mais aussi une animation !

L'utilisation d'un filtre sur un flux vid√©o est consid√©r√©e (ici) comme une **animation**. 

L'impl√©mentation du filtre et la modification des pixels font office de **m√©thode de rendu**, tandis que la synchronisation entre le canvas et le lecteur vid√©o d√©terminera le comportement de la **boucle de rendu**.

Ces termes vous paraissent abstraits ? J'ai √©crit un tutoriel sur ce sujet : [Fa√Ætes vos propres animations en JS](https://dev.to/qphilippot/faites-vos-propres-animations-en-js-34ok).


### Synchroniser l'animation avec le lecteur vid√©o - D√©finir la boucle de rendu

L'animation doit se lancer lorsqu'on clique sur play, s'arr√™ter √† la fin de la vid√©o ou quand appuie sur pause (pour ne pas rafra√Æchir une image qui ne change pas, ce serait dommage de g√¢cher des ressources CPU pour rien).
En d'autres termes, la boucle de rendu doit √™tre pilot√©e par le lecteur vid√©o.

Pour rappel, la **boucle de rendu** s'occupe de rafra√Æchir automatiquement notre canvas.

```js
const animation = new Animation({ /* ‚Ä¶ */ });

video.addEventListener('play', () => {
   animation.play();
});

video.addEventListener('pause', () => {
   animation.pause();
});

video.addEventListener('end', () => {
   animation.pause();
});

// render animation once when we click on timeline
video.addEventListener('timeupdate', () => {
   animation.askRendering()
});
```

### Impl√©mentation d'un filtre - D√©finir une m√©thode de rendu

Nous savons d√©sormais comment extraire les pixels d'une vid√©o et configurer la boucle de rendu. Il ne reste plus qu'√† d√©finir la m√©thode de rendu.

```js
const animation = new Animation({
    canvas: document.getElementById('tuto-canvas'),
    // rendering method is here
    render: (context, canvas) => {
        const imageData = extractVideoImageData(video, canvas.width, canvas.height);
        // apply filter over imageData here;
        animation.clear();
        context.putImageData(imageData, 0, 0);
        }
    }
);

```


Ce tutoriel s'appuie sur une connaissance rudimentaire des `canvas`. Besoin d'une piq√ªre de rappel ? Cet [article](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial/Pixel_manipulation_with_canvas) est un classique, de plus, il montre comment appliquer des filtres sur des images. Il constitue un excellent compl√©ment √† ce tuto. N'h√©sitez pas √† y jeter un ≈ìil  ! üëç



## R√©sum√© 

```js
import Animation from '../../shared/animation.model';

document.addEventListener('DOMContentLoaded', () => {
    // Create canvas for video's pixel extraction
    const extractPixelCanvas = document.createElement('canvas');
    const extractPixelContext = extractPixelCanvas.getContext('2d');

    function extractVideoImageData(video, width, height) {
        // avoid unnecessary resize as much as possible (optimization)
        if (extractPixelCanvas.width !==  width) {
            extractPixelCanvas.width =  width;
        }
    
        if (extractPixelCanvas.height !==  height) {
           extractPixelCanvas.height = height;
        }


       extractPixelContext.drawImage(video, 0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
       return extractPixelContext.getImageData(0, 0, extractPixelCanvas.width, extractPixelCanvas.height);
    }
    
    const video = document.getElementById('tuto-video');


    const animation = new Animation({
        canvas: document.getElementById('tuto-canvas'),
        render: (context, canvas) => {
            const imageData = extractVideoImageData(video, canvas.width, canvas.height);
            
            // apply filter over imageData here;
            
           animation.clear();
           context.putImageData(imageData, 0, 0);
       }
   });


    video.addEventListener('play', () => {
        animation.play();
    });

    video.addEventListener('pause', () => {
        animation.pause();
    });

    video.addEventListener('end', () => {
        animation.pause();
    });

    video.addEventListener('timeupdate', () => {
        animation.askRendering()
    })
});
```

## R√©sultat pr√©liminaire


![R√©cup√©ration du flux vid√©o et rendu dans un canvas en JS](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/no-filter-1.gif?raw=true)

*¬´ - Hein ? Je ne vois aucune diff√©rence‚Ä¶ ¬ª* üôà


Pr√©cis√©ment ! Nous n'avons encore appliqu√© aucun filtre. Toutefois, on constate que notre flux vid√©o est bel et bien r√©pliqu√© sans d√©formation ni latence.

Pour appliquer un filtre sur l'image, il suffira d'**appliquer un traitement sur l'`ImageData`** extraite dans la m√©thode de rendu.

Ce tutoriel pourrait s'arr√™ter l√† ; le m√©canisme n'est gu√®re plus compliqu√©. Toutefois, cela commence √† peine √† devenir cool, ne nous arr√™tons pas en si bon chemin ! 

# Exemple d'impl√©mentation de filtres en JS

## Grayscale

Nous allons simplement transformer les pixels `RGB` en niveau de gris.

```js
// get grayscale value for a pixel in buffer

function rgbToGrayscale(buffer, offset) {
   return Math.ceil((
       0.30 * buffer[offset] +
       0.59 * buffer[offset + 1] +
       0.11 * buffer[offset + 2]
   ) * (buffer[offset + 4] / 255.0));
}

/**
* @param {Uint8Array} pixelBuffer
*/
function applyGrayscaleFilter(pixelBuffer) {
   for (let offset = 0; offset <pixelBuffer.length; offset += 4) {
       const grayscale = rgbToGrayscale(pixelBuffer, offset);
       pixelBuffer[offset] = grayscale;
       pixelBuffer[offset + 1] = grayscale;
       pixelBuffer[offset + 2] = grayscale;
       pixelBuffer[offset + 3] = 255;
   }
}


const animation = new Animation({
   canvas: document.getElementById('tuto-canvas'),
   render: (context, canvas) => {
       const imageData = extractVideoImageData(video, canvas.width, canvas.height);
       applyGrayscaleFilter(imageData.data);
       
       animation.clear();
       context.putImageData(imageData, 0, 0);
   }
});
```


Nous rempla√ßons les canaux RGB de chaque pixel par leur niveau de gris.

Intuitivement, il serait tentant de calculer une moyenne des composantes `R`, `G` et `B` et d'utiliser cette valeur comme niveau de gris. Toutefois, l'≈ìil humain ne per√ßoit pas toutes les couleurs avec la m√™me sensibilit√©. Et puisque nous sommes plus sensibles √† certaines couleurs, il est tout naturel de donner plus d'importances √† celles-ci lors du calcul du niveau de gris.

Cela explique la pr√©sence des constantes `0.30`, `0.59` et `0.11` dans la m√©thode `rgbToGrayscale`. L'intensit√© obtenue par cette m√©thode est appel√©e la **luminance**  du pixel.


![Impl√©mentation du filtre grayscale en JS avec canvas ](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/grayscale.gif?raw=true)

## Supporter les interactions souris

Une animation, c'est bien. Mais une animation qui interagit avec la souris, c'est mieux ! Transformons le code afin d'invoquer `applyGrayscaleFilter` seulement quand le pointeur se trouve au dessus du canvas.

```js
const animation = new Animation({
    canvas: document.getElementById('tuto-canvas'),
    render: (context, canvas) => {
        const imageData = extractVideoImageData(video, canvas.width, canvas.height);
        
        // compute isPointerHoverCanvas ...

        if (isPointerHoverCanvas === false) {
            applyGrayscaleFilter(imageData.data);
        }
        
        animation.clear();
        context.putImageData(imageData, 0, 0);
    }
});
```

### D√©terminer la position de la souris par rapport au canvas


Il existe plusieurs fa√ßons de d√©terminer si le curseur est au-dessus d'un canvas. Selon l'approche, certaines sont plus appropri√©es que d'autres.

Dans cette situation, la plus simple consiste √† :
* R√©cup√©rer les coordonn√©es du canvas.
* Calculer sa **bo√Æte englobante** (ou **hitbox**)
* V√©rifier si les coordonn√©es du pointeur se trouvent dans la bo√Æte englobante ([collision AABB](https://developer.mozilla.org/en-US/docs/Games/Techniques/3D_collision_detection#point_vs._aabb)).

```js
const pointerCoords = {x: 0, y: 0};

document.addEventListener('pointermove', event => {
    pointerCoords.x = event.clientX;
    pointerCoords.y = event.clientY;
});

const animation = new Animation({
    canvas: document.getElementById('tuto-canvas'),
    render: (context, canvas) => {
        // ‚Ä¶

        const boundingBox = canvas.getBoundingClientRect();


        const isPointerHoverCanvas = (
            pointerCoords.x >= boundingBox.left &&
            pointerCoords.y >= boundingBox.top &&
            pointerCoords.x < boundingBox.right &&
            pointerCoords.y < boundingBox.bottom
        );


        if (isPointerHoverCanvas === false) {
            applyGrayscaleFilter(imageData.data);
        }
    }
});
```

![Filtre grayscale changeant lorsque le pointeur est sur le canvas en JS](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/grayscale-pointer-1.gif?raw=true)

## On corse le jeu ! üöÄ

On va appliquer le filtre grayscale sur toute l'image et ne faire appara√Ætre les couleurs que sur les pixels autour de notre curseur.

Petite subtilit√© : pour cr√©er un effet plus lisse, on d√©terminera un cercle √† l'int√©rieur duquel les pixels seront color√©s, mais avec une intensit√© inversement proportionnelle √† la distance au centre‚Ä¶

### Rappel g√©om√©trique
Un cercle peut √™tre d√©fini par un point (son centre), et un rayon. Dans notre cas, le centre du cercle correspond √† la position du pointeur. Quant au rayon, nous prendrons une valeur arbitraire.

D√©terminer si un point est dans un cercle revient √† calculer la **collision entre un point et un cercle**.

Pour en savoir plus sur les m√©thodes de collision : http://www.jeffreythompson.org/collision-detection/point-circle.php

### Approche g√©n√©rale

Pour chaque pixel, v√©rifions s'il est √† l'int√©rieur du cercle autour du pointeur. Afin de faciliter le calcul, nous allons nous placer dans le *rep√®re g√©om√©trique de notre canvas*. Les coordonn√©es ne seront plus exprim√©es en fonction de la page, mais de l'√©l√©ment `<canvas>`.

```js
render: (context, canvas) => {
    const imageData = extractVideoImageData(video, canvas.width, canvas.height);
    
    const coordsRelativeToCanvas = PointerCoordsHelper.getCoordsRelativeToElement(
        canvas,
        pointerCoords.x,
        pointerCoords.y
    );

    const buffer = imageData.data;
    
    // apply to the whole buffer, execept a circle defined by pointer position
    for (let offset = 0; offset < buffer.length; offset += 4) {
        const pixelOffset = (offset / 4); // pixels have 4 channel in ImageData
        const pixelX = pixelOffset % canvas.width;
        const pixelY = pixelOffset / canvas.width;

        // arbitrary radius
        const radius = 50;

        const isInCircle = CollisionHelper.isPointInCircle(
            pixelX, pixelY,
            coordsRelativeToCanvas.x, coordsRelativeToCanvas.y,
            radius
        );

        const grayscale = rgbToGrayscale(buffer, offset);

        if (isInCircle === false) {
            buffer[offset] = grayscale;
            buffer[offset + 1] = grayscale;
            buffer[offset + 2] = grayscale;
            buffer[offset + 3] = 255;
        } else {
            const distance = GeometryHelper.getDistanceBetween2DPoints(
                pixelX, pixelY,
                coordsRelativeToCanvas.x, coordsRelativeToCanvas.y
            );

            const weight = distance / radius;
            // apply a weight in order to let color intensity increase from the outside to the center
            buffer[offset] = weight * grayscale + (1 - weight) * buffer[offset];
            buffer[offset + 1] = weight * grayscale + (1 - weight) * buffer[offset + 1];
            buffer[offset + 2] = weight * grayscale + (1 - weight) * buffer[offset + 2];
            buffer[offset + 3] = 255;
        }
    }


    animation.clear();
    context.putImageData(imageData, 0, 0);
}
```


![Impl√©mentation d'un filtre r√©agissant selon la position du curseur en JS et canvas](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/grayscale-pointer-2.gif?raw=true)

<details>
<summary>‚ö†Ô∏è Remarque sur le calcul de coordonn√©es (Niveau avanc√©) ‚ö†Ô∏è</summary>

La position du curseur est exprim√©e relativement √† notre canvas (l'origine du rep√®re math√©matique est le coin sup√©rieur gauche du canvas). 

On aurait pu impl√©menter le filtre en utilisant directement les coordonn√©es du pointeur dans la fen√™tre (rep√®re standard), mais les √©quations auraient √©t√©s plus compliqu√©es. 

De plus, la r√©solution du canvas (*pixel th√©orique*) et sa taille (*pixel physique*) peuvent parfois varier. Puisque l'algorithme it√®re sur les pixels th√©oriques du canvas (`animation.context.width` ou `animation.canvas.width`), pour supporter correctement ce type de situation, il faudra modifier les √©quations pour prendre en compte ce changement de rep√®re suppl√©mentaire‚Ä¶

</details>

## Sa vision est bas√©e sur le mouvement ! ü¶ñ

Le filtre impl√©ment√© dans cet exemple n'affichera que les mouvements perceptibles entre deux frames. 

*¬´ - Calculer les mouvements ? √áa √† l'air difficile, non ? ¬ª*

Tout d√©pend ce que l'on appelle *mouvement*. Pour notre exemple, calculer la diff√©rence deux pixels entre deux frame est largement suffisant !

Le principe pour calculer le mouvement entre deux frame N-1 et N :
* Calculer le niveau de gris de la frame N-1
* Calculer le niveau de gris de la frame N
* Cr√©er une image en niveau de gris correspondant √† la valeur absolue de la diff√©rence des niveaux de gris des frame N et N - 1

Parce qu'un code vaut mieux que mille mots : 
```js
render: (context, canvas) => {
    const imageData = extractVideoImageData(video, canvas.width, canvas.height);
    const buffer = imageData.data;
  
    applyGrayscaleFilter(buffer);

    // first rendering
    if (lastBuffer === null) {
        lastBuffer = buffer.slice(0);
        window.lastBuffer = lastBuffer;
        return;
    }
    
    // compute movement
    const diffBuffer = new Uint8Array(buffer.length);
    
    for (let offset = 0; offset < buffer.length; offset += 4) {
        diffBuffer[offset] = Math.abs(buffer[offset] - window.lastBuffer[offset]);
        diffBuffer[offset + 1] = Math.abs(buffer[offset + 1] - window.lastBuffer[offset + 1]);
        diffBuffer[offset + 2] = Math.abs(buffer[offset + 2] - window.lastBuffer[offset + 2]);
        diffBuffer[offset + 3] = 255;
    }

    // update "last" buffer
    window.lastBuffer = buffer.slice(0);
    
    // overwrite image data in order to browse only the differences between the two frames
    diffBuffer.forEach((value, index) => {
        imageData.data[index] = value;
    });

    animation.clear();
    context.putImageData(imageData, 0, 0);
}
```

![D√©tection du mouvement entre deux frames avec JS et canvas](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/movement-1.gif?raw=true)

*¬´ - Mouais, avouons que c'est pas terrible... ¬ª* üòû

En effet, on peut mieux faire ! N√©anmoins, concentrons-nous sur le positif : nous avons un d√©but de quelque chose !

Nous parvenons √† d√©celer les **contours** du perroquet lorsqu'il effectue un mouvement. Mais ses d√©placements sont lents, peu perceptibles d'une frame sur l'autre. 

De plus, le **taux de rafraichissement** √©tant relativement √©lev√© (60 fps), nous effectuons un rendu approximativement toutes les 16ms. Les mouvements ne sont donc perceptibles que durant ce laps et temps et sont oubli√©s au rendu suivant.

Sachant que la **persistance r√©tinienne** est de l'ordre de 1/25 de secondes (40 ms), pour avoir un rendu plus fid√®le, il faudrait garder en m√©moire l'image des 40 derni√®res ms, et les prendre en compte dans notre calcul du mouvement.

### Am√©lioration simple

Plut√¥t que de se lancer dans un calcul p√©rilleux sur le taux de rafra√Æchissement optimal, nous allons opter pour une solution b√™te et m√©chante : calculer le mouvement en prenant en compte, non pas la derni√®re frame, mais les X derni√®res frames.

```js
function computeMovement(target, newFrame, oldFrame) {
    let offset = 0;
    const length = newFrame.length;
    
    // another version of for-loop to compute movement
    while (offset < length) {
        target[offset] = Math.abs(newFrame[offset] - oldFrame[offset]);
        target[offset + 1] = Math.abs(newFrame[offset + 1] - oldFrame[offset + 1]);
        target[offset + 2] = Math.abs(newFrame[offset + 2] - oldFrame[offset + 2]);
        offset += 4;
    }
}

/**
 * check previous frame difference and apply a weight 
 * @return Uint8Array buffer with some extra movement pixel to add
 */
function computePersistance(buffer) {
    /*** @var {Number} historyLength is a global var, it's the number of frame to consider ***/
    let indexedHistoryBuffer = Array(historyLength);
    let weights = Array(historyLength);
    
    for (let k = 0; k < historyLength; k++) {
        indexedHistoryBuffer[k] = getHistoryBuffer(k);
        weights[k] = state.persistanceFactor * (k / historyLength);
    }

    const length = buffer.length;
    let pixelOffset = 0;
    let historyBufferOffset, historyBuffer;
    let c1, c2, c3, c4;
    
    while (pixelOffset < length) {
        c1 = pixelOffset;
        c2 = c1 + 1;
        c3 = c2 + 1;
        c4 = c3 + 1;

        buffer[pixelOffset] = 0;
        buffer[c2] = 0;
        buffer[c3] = 0;
        buffer[c4] = 255;

        historyBufferOffset = historyLength - 1;
        
        while (historyBufferOffset >= 0) {
            historyBuffer = indexedHistoryBuffer[historyBufferOffset];
            buffer[pixelOffset] += weights[historyBufferOffset] * historyBuffer[pixelOffset];
            buffer[c2] += weights[historyBufferOffset] * historyBuffer[c2];
            buffer[c3] += weights[historyBufferOffset] * historyBuffer[c3];

            historyBufferOffset--
        }
        
        pixelOffset++;
    }
}
```
**Remarques :**
* Afin d'appliquer notre *facteur de persistance* on se base directement sur les diff√©rences calcul√©es lors des rendus pr√©c√©dents.
* Pour √©viter d'instancier une trop grande quantit√© de buffers, nous utilisons un pool d'instances que nous g√©rons gr√¢ce √† `getHistoryBuffer` 


```js
render: (context, canvas) => {
    const imageData = extractVideoImageData(video, canvas.width, canvas.height);
    const buffer = imageData.data;
    applyGrayscaleFilter(buffer);

    // first rendering
    if (lastBuffer === null) {
        lastBuffer = buffer.slice(0);
        window.lastBuffer = lastBuffer;
        return;
    }
    
    const diffBuffer = new Uint8Array(buffer.length);
    const persistanceBuffer = new Uint8Array(buffer.length);
    
    computeMovement(diffBuffer, buffer, window.lastBuffer);
    computePersistance(persistanceBuffer);

    shallowCopy(lastBuffer, buffer);

    // clamp sum of diffs 
    for (let offset = 0; offset < buffer.length; offset += 4) {
        buffer[offset] = Math.ceil(Math.min(255, diffBuffer[offset] + persistanceBuffer[offset]));
        buffer[offset + 1] = Math.ceil(Math.min(255, diffBuffer[offset + 1] + persistanceBuffer[offset + 1]));
        buffer[offset + 2] = Math.ceil(Math.min(255, diffBuffer[offset + 2] + persistanceBuffer[offset + 2]));
        buffer[offset + 3] = 255;
    }

    let currentHistoryBuffer = diffHistory[state.currentOffset];
    shallowCopy(currentHistoryBuffer, diffBuffer);
    
    state.currentOffset = nbFrameRendered % historyLength;
    

    animation.clear();
    context.putImageData(imageData, 0, 0);
}
```
**Remarques :**
* Lorsqu'on additionne des buffers, ne pas oublier d'effectuer un **clamp** afin de s'assurer que les valeurs additionn√©es restent dans l'intervalle des valeurs autoris√©es par la structure de donn√©e (entre 0 et 255).
* La m√©thode `shallowCopy` se contente d'effectuer une **copie superficielle** d'un tableau dans un autre tableau. Le but est de r√©utiliser les instances existantes et d'√©viter la r√©p√©tition de code. Son impl√©mentation est triviale et disponible sur le [git](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/). 

![D√©tection de mouvement gr√¢ce √† un filtre sur un canvas JS avec simulation de persistance](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/movement-2.gif?raw=true)


Voici une tentative se basant sur les 5 derni√®res frames, avec coefficient √©quivalent √† 15. Si les mouvements sont davantages perceptibles, le co√ªt en calcul lui est nettement plus √©lev√©. On passe de 60 fps √† un peu plus de 20 fps. Rien de plus normal, on a presque tripl√© la charge de travail.

Il existe des m√©thodes permettant d'obtenir un r√©sultat plus propre et moins gourmant en calcul, mais √©galement moins simple √† expliquer üòÅ. 

Puisque le but de cet article est de pr√©senter des filtres simples, je les passe sous silence. Cela fera peut-√™tre l'objet d'un prochain tutoriel.
## La vie en bleu ü¶ú

Dans ce dernier exemple, je propose de teindre ce cher perroquet en bleu.

Pour atteindre notre objectif, consid√©rons la couleur de son plumage d'origine. Il n'est pas simplement rouge, mais couvre une nuance de rouge. Le filtre devra prendre en compte toutes ces nuances, pour proposer un rendu r√©aliste prenant en compte la pigmentation naturelle des plumes ainsi que les variations de luminosit√©.

### Rappel sur la repr√©sentation de la couleur
La repr√©sentation des couleurs dans les `ImageData` est en `RGBA`. En d'autres termes, la couleur finale est obtenue √† partir d'un m√©lange des quatre composantes.

Une solution na√Øve consisterait √† supprimer la dimension rouge (mettre toutes les intensit√©s √† 0). Le d√©faut de cette repr√©sentation (`RGBA`), toutes les couleurs ont une part contiennent une part de rouge. Autrement dit, si l'on modifie la composante `R`, quasiment toutes les couleurs seront impact√©es.

Bonne nouvelle : il existe √©norm√©ment d'espaces couleurs et dont la plupart ne sont pas coupl√©s √† la couleur rouge ! Des formules math√©matiques permettent de changer facilement de repr√©sentation, il n'y a donc aucune raison de se borner √† ce bon vieux `RGB`.

Selon le cas d'usage, certains espaces couleurs sont plus pratiques que d'autres (`YCrCb` pour la compression, `CMJN` pour l'impression, etc).

Dans le cas pr√©sent, l'ensemble `HSL` *Hue Saturation Lightness*, ou `TSV` en fran√ßais semble le plus appropri√©. Dans cet espace, la **teinte** des couleurs est d√©finie via un cercle colorim√©trique.

Pour transformer du "rouge" en "bleu", il suffit de d√©terminer une section du cercle que l'on souhaite remplacer et d'y coller la section par laquelle on souhaite le remplacer.

![Sch√©ma illustrant l'espace TSV](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/HSV_cone.png/300px-HSV_cone.png)

### Principe du filtre

* R√©cup√©rer la couleur des pixels `RGBA`.
* Les convertir en `HSL`.
* Manipuler les teintes *rouges* et les remplacer par des *bleues*.
* Reconvertir en `RGBA`.
* Remplir l'instance `ImageData` avec les pixels modifi√©s.

### Impl√©mentation

Concernant les fonctions de transformations `HSL` vers `RGBA` et r√©ciproquement, je vous laisse checker le [git](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-vide).

Pour des raisons de performances, nous allons impl√©menter une **Look Up Table (*LUT*)**, c'est-√†-dire une table de correspondance pour toutes nos couleurs. L'enjeu est de ne pas calculer toutes les correspondances de couleurs √† la vol√©e (pixels par pixels √† chaque rendu), mais de les calculer une bonne fois pour toutes au lancement de la page. 

La m√©thode de rendu n'aura qu'√† lire dans cette LUT pour y lire les r√©sultats et gagner un temps pr√©cieux (et un meilleur frame rate).
#### Calcul de la LUT
```js
function generateRedToBlueLUT() {
    const size = 16777216; // 256 * 256 * 256
    const lut = new Array(size);

    // initialize all colors to black
    for (let i = 0; i < size; i++) {
        lut[i] = [0, 0, 0];
    }

    // iterate through RGB combinaisons
    for (let redOffset = 0; redOffset < 256; redOffset++) {
        for (let greenOffset = 0; greenOffset < 256; greenOffset++) {
            for (let blueOffset = 0; blueOffset < 256; blueOffset++) {
                // Use a pool design pattern
                // If you want to implements it without object pool, juste replace it by [0, 0, 0]
                const rgb = vec3Pool.getOne();
                const hsl = vec3Pool.getOne();
                
                rgb[0] = redOffset;
                rgb[1] = greenOffset;
                rgb[2] = blueOffset;

                // color conversion, check sources for detailled implementation 
                rgbToHSL(rgb, hsl);

                // Clamp saturation and lightness
                hsl[1] = Math.max(0, Math.min(hsl[1], 1));
                hsl[2] = Math.max(0, Math.min(hsl[2], 1));

                // Here is the trick: hue is represented by a degree angle
                // We want : 0 <= hue < 360
                if (hsl[0] < 0) {
                    hsl[0] += 360;
                }

                hsl[0] = hsl[0] % 360;

                // Assume that :
                // - "red" hues are between 340¬∞ and 20¬∞
                // - "blue" are between 140¬∞ and 220¬∞
                
                // replace hue
                if (hsl[0] > 340 && hsl[2] < 0.85) {
                    hsl[0] -= 120;
                }

                else if (hsl[0] < 20 && hsl[2] < 0.85) {
                    hsl[0] += 240;
                }

                // sanitize angle : 0 <= hue < 360 
                if (hsl[0] < 0) {
                    hsl[0] += 360;
                }

                hsl[0] = hsl[0] % 360;
                
                hslToRGB(hsl, rgb);

                // store RGBA converted into lut
                lut[redOffset * 65536 + greenOffset * 256 + blueOffset] = Array.from(rgb);
                
                // recycle instance, only for object pool implementation
                vec3Pool.recycle(rgb);
                vec3Pool.recycle(hsl);
            }
        }
    }

    return lut;
}
window.lut = generateRedToBlueLUT();
 ```
Plusieurs remarques sur cette impl√©mentation :
* Notre LUT est un tableau. On calcule l'index de chaque couleur par la formule `R * 255 * 255 + G * 255 + B`
* Pour des raisons de performances, on utilise un object pool design pattern. Le calcul d'une LUT demande d'instancier pas mal de petits tableaux, cela peut surcharger inutilement la m√©moire du navigateur. Pour en savoir plus sur l'impl√©mentation de l'object pool design pattern en JS, lisez l'article suivant : [Optimisez vos applications JS avec l'Object Pool Design Pattern !](https://dev.to/qphilippot/optimisez-vos-applications-js-avec-l-object-pool-design-pattern-3g8)
* Les calculs d'angles sont empiriques, √† partir du cercle colorim√©trique. D'ailleurs, en regardant attentivement le rendu, on peut s'apercevoir que la "teinture" n'est pas parfaite et que quelques pointes de rouges se prom√®nent √ßa et l√† üòâ

#### Coup d'oeil sur la m√©thode de rendu

```js
render: (context, canvas) => {
    const imageData = extractVideoImageData(video, canvas.width, canvas.height);
    const buffer = imageData.data;

    for (let offset = 0; offset < buffer.length; offset += 4) {
        const r = buffer[offset];
        const g = buffer[offset + 1];
        const b = buffer[offset + 2];

        // 65536 = 256 * 256
        const lutIndex = r * 65536 + g * 256 + b;
        
        // just replace color by pre-computed value
        const color = window.lut[lutIndex];

        buffer[offset] = color[0];
        buffer[offset + 1] = color[1];
        buffer[offset + 2] = color[2];
        buffer[offset + 3] = 255;

    }
    
    animation.clear();
    context.putImageData(imageData, 0, 0);
}
```

Et voici un beau perroquet haut en couleur ! :D

![Perroquet bleu apr√®s l'application d'un filtre en JS](https://github.com/qphilippot/tuto/blob/master/apply-filter-on-video/assets/gif/blue-1.gif?raw=true)

## Conclusion

J'esp√®re sinc√®rement que ce tutoriel vous a plu. Le principe derri√®re l'utilisation de filtres en live est assez simple √† impl√©menter, mais n√©cessitait bien selon moi quelques exemples pour comprendre son utilisation. Je suis pass√© assez rapidement sur certains points pour √©viter de d√©vier du sujet principal : ~~torturer ce pauvre oiseau~~ utiliser une boucle de rendu pour appliquer des filtres en temps r√©els.

N'h√©sitez pas √† me faire part de vos commentaires ou de vos remarques, c'est toujours un plaisir üòâ